
# ğŸ§  Keyword Extraction using TF-IDF + Word2Vec  
**Core Concept:** Hybrid NLP + Statistical Approach  
**Tech Stack:** Python, scikit-learn, gensim, NLTK, FastAPI



##  Overview
This project performs **automatic keyword extraction** from articles using a **hybrid approach** that combines:
- **TF-IDF (Term Frequencyâ€“Inverse Document Frequency):** Captures statistical importance of words across documents.  
- **Word2Vec:** Learns semantic relationships between words.

The final system allows users to send raw text to a **FastAPI endpoint** and get the most relevant keywords in response.



## ğŸ§© Features
âœ… Preprocessing pipeline (tokenization, stopword removal, punctuation cleanup)  
âœ… TF-IDF vectorization for statistical weighting  
âœ… Word2Vec embedding training for semantic understanding  
âœ… REST API endpoint (`/extract_keywords`) for real-time keyword extraction  
âœ… Deployment-ready structure (FastAPI + Uvicorn)  


## ğŸ—ï¸ Project Structure
keyword_extraction/
â”‚
â”œâ”€â”€ data/                           # Contains CSV files with categorized articles
â”‚   â”œâ”€â”€ business_data.csv
â”‚   â”œâ”€â”€ education_data.csv
â”‚   â”œâ”€â”€ entertainment_data.csv
â”‚   â”œâ”€â”€ sports_data.csv
â”‚   â””â”€â”€ technology_data.csv
â”‚
â”œâ”€â”€ keyword_extraction.ipynb         # Notebook for training and saving models
â”œâ”€â”€ main.py                          # FastAPI app for deployment
â”œâ”€â”€ tfidf_vectorizer.pkl             # Saved TF-IDF vectorizer
â”œâ”€â”€ w2v_model.model                  # Saved Word2Vec model
â”œâ”€â”€ requirements.txt                 # Project dependencies
â””â”€â”€ README.md                        # Project documentation


## âš™ï¸ Installation

### 1ï¸âƒ£ Clone the Repository

git clone https://github.com/yourusername/keyword-extraction.git
cd keyword-extraction


### 2ï¸âƒ£ Create and Activate Virtual Environment

py -3.12 -m venv keyword_extraction_venv
source keyword_extraction_venv/Scripts/activate   # For Windows (Git Bash)


### 3ï¸âƒ£ Install Dependencies

pip install -r requirements.txt


## ğŸ§  Model Training (Optional)

If you want to train your own models on new articles:

1. Place your `.csv` files (containing article text) in the `data/` folder.
2. Open and run `keyword_extraction.ipynb` to preprocess data, train models, and save:

   * `tfidf_vectorizer.pkl`
   * `w2v_model.model`


## ğŸŒ API Deployment

### Run FastAPI Locally
uvicorn main:app --reload

Then open:
ğŸ‘‰ [http://127.0.0.1:8000/docs](http://127.0.0.1:8000/docs)

Youâ€™ll see the Swagger UI where you can test the endpoint `/extract_keywords`.

### Example Request

{
  "text": "Artificial intelligence is revolutionizing modern software and redefining cybersecurity."
}


### Example Response


{
  "keywords": ["artificial", "intelligence", "cybersecurity", "modern", "software"]
}

## ğŸ§° Technologies Used

| Category      | Tools                      |
| ------------- | -------------------------- |
| Programming   | Python 3.12                |
| NLP Libraries | NLTK, gensim, scikit-learn |
| API Framework | FastAPI                    |
| Deployment    | Uvicorn                    |
| Vector Models | TF-IDF, Word2Vec           |



## ğŸ’¡ Future Improvements

* Integrate BERT embeddings for context-aware keyword extraction
* Add a front-end dashboard for visualization
* Deploy on Render or Hugging Face Spaces for public access



## ğŸ‘©â€ğŸ’» Author

**Monika Chaulagain**
ğŸ“« Reach me at: chaulagainmonika2005@gmail.com



## ğŸ“œ License

This project is open-source and available under the **MIT License**.
